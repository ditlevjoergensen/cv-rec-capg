{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing specific issues with the CV's\n",
    "Some testing... Sara's failed due to the \"ć\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sara Vera Marjanović\n",
      "Data Scientist\n",
      "Data scientist\n",
      "Sara is a driven data scientist with a particular specialty in Machine Learning, Deep learning, and Natural Language \n",
      "Processing. She has end-to-end experience with wrangling a wide variety of different data sources (from free text to \n",
      "biometric data) to create either simple (e.g. clustering, regression) and complex models (e.g. Generative Adversarial \n",
      "Models) in the production pipeline. Therefore, she is well-versed in ideation, data collection, data wrangling and \n",
      "exploration, as well as model development and trouble-shooting and is comfortable with Python (especially the libraries \n",
      "Numpy, Pandas, Tensorflow, Pytorch) as well as SQL, R, and Azure DevOps.\n",
      "Within her model creation and data exploration, Sara takes special care to limit bias within the dataset and model output, \n",
      "and to ensure interpretability of the results. She is diligent and conscientious with her data storytelling and data \n",
      "visualization, making her a skilled communicator and heads into each project with energy and enthusiasm. She is capable \n",
      "of collaborating within a team as well as leading assignments even on unfamiliar terrain as she is a quick and independent \n",
      "learner.\n",
      "ENGAGEMENTS\n",
      "10.2021 - 06.2022 Mental Health Consulting Firm, Personalized Insights\n",
      "Project description: The client aimed to provide their end-clients with personalized understanding\n",
      "of their workplace stressors. Biometric data (from wearable Garmin devices) was combined with \n",
      "Outlook calendar data to find correlations and predictive features of high stress personalized to \n",
      "each end-client.\n",
      "Role: AI Engineer\n",
      "Responsibilities: Sara solely developed the final solution for this project. She wrangled biometric, \n",
      "free text and boolean data from the various data sources to select relevant features for the model.\n",
      "She implemented an automated notifications system on the client's app to notify end-clients of \n",
      "personalized correlations to high stress.\n",
      "Skills: Python, Azure Functions, Feature Engineering, Digital Signal Processing, API Integration, \n",
      "SQL, Natural Language Processing, Azure Cloud\n",
      "10.2021 - 06.2022 Mental Health Consulting Firm, Insights Reports\n",
      "Project description: This client provided a mental health-related consulting services to their end-\n",
      "clients; this service was supplemented with regular reports showing the progression of the \n",
      "program and its effect on various data points (e.g. measured stress levels, recorded sleep \n",
      "duration, reported feedback, app usage).\n",
      "CV | Sara \n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "with pdfplumber.open(\"C:\\Projects\\Staffing\\data\\CV's\\Sara_Vera_Marjanovi_.pdf\") as pdf:\n",
    "    print(pdf.pages[0].extract_text()[:-60])\n",
    "    #for j, page in enumerate(pdf.pages):\n",
    "    #    print(page.extract_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting text as string from pdf', save to .csv\n",
    "Here we read the text from all CV's and save the texts in a .csv file. Each line in the .csv contains name of the colleague, the page number of the CV and the text on that specific page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently at path: Antonio_Arfe\n",
      "Currently at path: Ashish_Meshram\n",
      "Currently at path: Ayman_Eleya_Zaki\n",
      "Currently at path: Aziz_Ari\n",
      "Currently at path: Branislav_Machava\n",
      "Currently at path: Daniel_Masson\n",
      "Currently at path: Daria_Korzel\n",
      "Currently at path: Devichand_Das\n",
      "Currently at path: Dinesh_Kumar_Padala\n",
      "Currently at path: Ditlev_J_rgensen\n",
      "Currently at path: Drisya_Thumba\n",
      "Currently at path: Emil_Djursner_Rasmussen\n",
      "Currently at path: Eugenia_Borgia\n",
      "Currently at path: Hamza_Minhas\n",
      "Currently at path: Hatef_Abdollahi\n",
      "Currently at path: Jacob_Karlstr_m\n",
      "Currently at path: Jan_Neldeberg\n",
      "Currently at path: Jayant_Pant\n",
      "Currently at path: Jes_Melbye_Chergui\n",
      "Currently at path: Julian_Eikhaug\n",
      "Currently at path: Kristina_Kj_r_Hedegaard\n",
      "Currently at path: Ksheeraja_Vinjanampati_\n",
      "Currently at path: Lee_Ann_Zambrano_Larsen\n",
      "Currently at path: Line_Bruun\n",
      "Currently at path: Mohd_Arshad\n",
      "Currently at path: Mohit_Kumar_Bhati\n",
      "Currently at path: Mostafa_Ellabaan\n",
      "Currently at path: Olivia_Essen\n",
      "Currently at path: Peter_Damm\n",
      "Currently at path: Prashanthy_Ramanan\n",
      "Currently at path: Rajitha_Dodda\n",
      "Currently at path: Rakesh_Ghosh\n",
      "Currently at path: Rasmus_Rosenmeyer_Poulsen\n",
      "Currently at path: Saket_Kumar\n",
      "Currently at path: Samay_Mir\n",
      "Currently at path: Sara_Vera_Marjanovi_\n",
      "Currently at path: Shabina_Bibi\n",
      "Currently at path: Shari_Kizhakke_Neelamana\n",
      "Currently at path: Shivam_Singh\n",
      "Currently at path: Simon_Troelsg_rd\n",
      "Currently at path: Steen_Ostersen\n",
      "Currently at path: S_ren_Andersen\n",
      "Currently at path: Theresa_Claudia_Eschelbach\n",
      "Currently at path: Ting_Yu__Terry__Kuo\n",
      "Currently at path: Varun_Singh\n",
      "Currently at path: Verma_Kanika\n",
      "Currently at path: Vicky_Singh_Kumar\n",
      "Currently at path: Viktor_Ekstr_m\n",
      "Currently at path: Yumeng_Ming\n",
      "Currently at path: Zhong_Guan\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import glob\n",
    "import regex as re\n",
    "import pdfplumber\n",
    "# Get list of CV paths\n",
    "paths = glob.glob(\"C:\\Projects\\Staffing\\data\\CV's\\*.pdf\")\n",
    "#print(paths)\n",
    "\n",
    "with open(\"cv_test.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    csv_input = [\"colleague\", \"page_number\", \"page_text\"]\n",
    "    writer.writerows([csv_input])\n",
    "    for i, path in enumerate(paths):\n",
    "        print(f\"Currently at path: {path[31:-4]}\")\n",
    "        with pdfplumber.open(path) as pdf:\n",
    "            for j, page in enumerate(pdf.pages):\n",
    "                # Only special characters are removed.\n",
    "                csv_input = [path[31:-4],j+1,re.sub('[^A-Za-z0-9 .,]+', '', page.extract_text())]\n",
    "                #Write page text to pdf\n",
    "                writer.writerows([csv_input])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TaggedDocument, train Doc2Vec model on data\n",
    "First we created a TaggedDocument object where we tag each document with the respective name. Each document is a page on the CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument<['Antonio', 'ArfeSenior', 'Big', 'Data', 'EngineerSenior', 'Data', 'Engineer', 'Antonio', 'is', 'an', 'experienced', 'SoftwareData', 'Engineer', 'interested', 'in', 'anything', 'technically', 'challenging', '.', 'He', 'has', 'worked', 'on', 'a', 'wide', 'range', 'of', 'IT', 'projects', 'at', 'Enterprise', 'level', ',', 'gathering', 'a', 'solid', 'experience', 'in', 'productionizing', 'infrastructure', 'applications', 'and', 'data', 'pipelines', 'to', 'support', 'Data', 'Scientists.He', 'worked', 'on', 'two', 'main', 'areas', 'Design', ',', 'implement', 'and', 'maintain', 'infrastructure', 'applications', 'that', 'should', 'run', 'for', 'years', ',', 'be', 'highly', 'available', 'and', 'fault', 'tolerant', 'Build', 'data', 'pipelines', 'and', 'productionize', 'advanced', 'analytics', 'models', 'in', 'constant', 'need', 'of', 'changes', 'and', 'new', 'featuresHIGHLIGHTS', 'Explain', 'any', 'kind', 'of', 'Distributed', 'software', 'system', 'to', 'technical', 'and', 'nontechnical', 'audiences', '.', 'Help', 'data', 'scientists', 'productionize', 'their', 'models', 'Catch', 'errors', 'before', 'implementation', 'starts', 'CICD', 'mechanisms', 'Teamwork', 'in', 'international', 'environmentENGAGEMENTS05.2020', 'Worldwide', 'furniture', 'retailer', ',', 'Customer', 'Support', 'IntelligenceProject', 'description', 'Productionise', 'AIML', 'infrastructure', 'and', 'model', 'pipelines', 'for', 'the', 'Customer', 'Support', 'Intelligence', 'teamRole', 'Senior', 'Data', 'EngineerResponsibilities', 'Productionise', 'MLAI', 'pipelines', ',', 'from', 'sourcing', 'data', 'to', 'creating', 'insights', 'Extract', 'Manipulate', 'data', 'on', 'the', 'Cloud', 'to', 'make', 'them', 'available', 'to', 'BI', 'dashboards.Skills', 'Google', 'Cloud', 'Build', ',', 'Google', 'Cloud', 'Functions', ',', 'GCP', 'Cloud', 'AI', ',', 'Google', 'Cloud', 'Composer', ',', 'GoogleCloud', 'Dataflow', ',', 'Apache', 'Airflow', ',', 'Google', 'BigQuery', ',', 'CICD03.2020', '04.2020', 'Capgemini', 'Danmark', 'AS', ',', 'COVID', '19', 'DashboardProject', 'description', 'Solution', 'to', 'extract', 'new', 'Insights', 'and', 'Visuualizations', 'from', 'the', 'official', 'COVID', 'data', 'released', 'by', 'the', 'authorities.Role', 'Senior', 'Data', 'EngineerCV', 'Antonio', 'Arfe', '2022', 'Capgemini', '.', 'All', 'rights', 'reserved', '.', '17'], ['Antonio_Arfe']>\n",
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec,TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"cv_test.csv\")\n",
    "\n",
    "tagged_data = []\n",
    "cnt = 0\n",
    "for tags in df[\"colleague\"].tolist():\n",
    "    for doc in df[\"page_text\"].tolist():\n",
    "        tagged_data.append(TaggedDocument(words=word_tokenize(doc), tags=[cnt, str(tags)]))\n",
    "    \n",
    "    cnt += 1\n",
    "\n",
    "print(tagged_data[0])\n",
    "\n",
    "#tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(name)]) for name, _d in (df[\"colleague\"], df[\"page_text\"])]\n",
    "max_epochs = 20\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(vector_size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=5,\n",
    "                dm =1)\n",
    "\n",
    "model.build_vocab(tagged_data) # tagged_data\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data, # tagged_data\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.epochs)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v_cvs.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use trained model to find best match between a subject and a colleague.\n",
    "Here we use the trained model to match which CV best fits the given text (subject tested on is Data Migration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n",
      "[('Prashanthy_Ramanan', 0.685684859752655), ('Hamza_Minhas', 0.6856452226638794), ('Ditlev_J_rgensen', 0.6827963590621948), ('Ashish_Meshram', 0.6826142072677612), ('Line_Bruun', 0.6804088950157166), ('Saket_Kumar', 0.6799563765525818), ('Kristina_Kj_r_Hedegaard', 0.6793414354324341), ('Peter_Damm', 0.6740396618843079), ('Daria_Korzel', 0.6727231740951538), ('Mohd_Arshad', 0.6713230609893799)]\n",
      "Prashanthy_Ramanan\n",
      "TaggedDocument<['Antonio', 'ArfeSenior', 'Big', 'Data', 'EngineerSenior', 'Data', 'Engineer', 'Antonio', 'is', 'an', 'experienced', 'SoftwareData', 'Engineer', 'interested', 'in', 'anything', 'technically', 'challenging', '.', 'He', 'has', 'worked', 'on', 'a', 'wide', 'range', 'of', 'IT', 'projects', 'at', 'Enterprise', 'level', ',', 'gathering', 'a', 'solid', 'experience', 'in', 'productionizing', 'infrastructure', 'applications', 'and', 'data', 'pipelines', 'to', 'support', 'Data', 'Scientists.He', 'worked', 'on', 'two', 'main', 'areas', 'Design', ',', 'implement', 'and', 'maintain', 'infrastructure', 'applications', 'that', 'should', 'run', 'for', 'years', ',', 'be', 'highly', 'available', 'and', 'fault', 'tolerant', 'Build', 'data', 'pipelines', 'and', 'productionize', 'advanced', 'analytics', 'models', 'in', 'constant', 'need', 'of', 'changes', 'and', 'new', 'featuresHIGHLIGHTS', 'Explain', 'any', 'kind', 'of', 'Distributed', 'software', 'system', 'to', 'technical', 'and', 'nontechnical', 'audiences', '.', 'Help', 'data', 'scientists', 'productionize', 'their', 'models', 'Catch', 'errors', 'before', 'implementation', 'starts', 'CICD', 'mechanisms', 'Teamwork', 'in', 'international', 'environmentENGAGEMENTS05.2020', 'Worldwide', 'furniture', 'retailer', ',', 'Customer', 'Support', 'IntelligenceProject', 'description', 'Productionise', 'AIML', 'infrastructure', 'and', 'model', 'pipelines', 'for', 'the', 'Customer', 'Support', 'Intelligence', 'teamRole', 'Senior', 'Data', 'EngineerResponsibilities', 'Productionise', 'MLAI', 'pipelines', ',', 'from', 'sourcing', 'data', 'to', 'creating', 'insights', 'Extract', 'Manipulate', 'data', 'on', 'the', 'Cloud', 'to', 'make', 'them', 'available', 'to', 'BI', 'dashboards.Skills', 'Google', 'Cloud', 'Build', ',', 'Google', 'Cloud', 'Functions', ',', 'GCP', 'Cloud', 'AI', ',', 'Google', 'Cloud', 'Composer', ',', 'GoogleCloud', 'Dataflow', ',', 'Apache', 'Airflow', ',', 'Google', 'BigQuery', ',', 'CICD03.2020', '04.2020', 'Capgemini', 'Danmark', 'AS', ',', 'COVID', '19', 'DashboardProject', 'description', 'Solution', 'to', 'extract', 'new', 'Insights', 'and', 'Visuualizations', 'from', 'the', 'official', 'COVID', 'data', 'released', 'by', 'the', 'authorities.Role', 'Senior', 'Data', 'EngineerCV', 'Antonio', 'Arfe', '2022', 'Capgemini', '.', 'All', 'rights', 'reserved', '.', '17'], ['Antonio_Arfe']>\n"
     ]
    }
   ],
   "source": [
    "model= Doc2Vec.load(\"d2v_cvs.model\")\n",
    "print(\"Model loaded!\")\n",
    "\n",
    "\n",
    "# Test random text on data migration\n",
    "doc = \"\"\"Data Migration\n",
    "Data-Migration-Icon-603x603What is Data Migration?\n",
    "Data Migration is the process of selecting and moving data from one location to another – this may involve moving data across different storage vendors, and across different formats.\n",
    "\n",
    "Data migrations are often done in the context of retiring a system and moving to a new system, or in the context of a cloud migration, or in the context of a modernization or upgrade strategy.\n",
    "\n",
    "Data migrations can be laborious, error prone, manual, and time consuming. Migrating data may involve finding and moving billions of files, which can succumb to storage and network slowdowns or outages. Also, different file systems do not often preserve metadata in exactly the same way, so migrating data without loss of fidelity and integrity can be a challenge.\n",
    "\n",
    "Network Attached Storage (NAS) migration is the process of migrating from one NAS storage environment to another. This may involve migrations within a vendor’s ecosystem such as NetApp data migration to NetApp or across vendors such as NetApp data migration to Isilon or EMC to NetApp or EMC to Pure FlashBlade. A high-fidelity NAS migration solution should preserve not only the file itself but all of its associated metadata and access controls.\n",
    "\n",
    "Network Attached Storage (NAS) to Cloud data migration is the process of moving data from an on-premises data center to a cloud. It requires data to be moved from a file format (NFS or SMB) to an Object/Cloud format such as S3. A high-fidelity NAS-to-Cloud migration solution preserves all the file metadata including access control and privileges in the cloud. This enables data to be used either as objects or as files in the cloud.\n",
    "\n",
    "Storage migration is a general-purpose term that applies to moving data across storage arrays.\n",
    "\n",
    "Data migrations typically involve four phases:\n",
    "\n",
    "Planning – Deciding what data should be migrated. Planning may often involve analyzing various sources to find the right data sets. For example, several customers today are interested in upgrading some data to Flash – finding hot, active data to migrate to Flash can be a useful planning exercise.\n",
    "Initial Migration – Do a first migration of all the data. This should involve migrating the files, the directories and the shares.\n",
    "Iterative Migrations – Look for any changes that may have occurred during the initial migration and copy those over.\n",
    "Final Cutoff – A final cutoff involves deleting data at the original storage and managing the mounts, etc., so data can be accessed from the new location going forward.\n",
    "Resilient data migration refers to an approach that automatically adjusts for failures and slowdowns and retries as needed. It also checks the integrity of the data at the destination to ensure full fidelity.\n",
    "\n",
    "Types of data migration\n",
    "When it comes to file data, there are NAS Migrations and Cloud Migrations. There are also NAS migrations to the cloud. Data migrations are often seen as a dreaded and laborious part of the storage management lifecycle. Free tools are often considered first but they can introduce risk, time and cost overruns and they are typically labor intensive and error-prone. On the other hand, traditional migration tools have complex legacy architectures and are expensive point products that do not provide ongoing value – resulting in sunk costs.\n",
    "\n",
    "Look for simple, fast, reliable data migration tools are not one-and-done point tools. The right data migration solution should be able to handle other data management use cases, including data tiering and data replication.\n",
    "\n",
    "How to plan a Smart NAS or Cloud data migration?\n",
    "The typical steps for any data migration project are:\n",
    "\n",
    "Analytics: Before you start an unstructured data migration project, it’s important to have visibility into:\n",
    "\n",
    " How fast is your data growing?\n",
    " How much data is hot vs. cold cold?\n",
    " Who is using your data?\n",
    "Savings: Estimate how much you’ll save by moving to the new NAS or cloud infrastructure. This information will guide which NAS or cloud storage mix is best for your data.\n",
    "\n",
    "Offload heavy lifting: Your data migration solution should be able to manage multiple iterations of the migration and handle problems by automatically retrying in a\n",
    "slowdown or a network or storage failure.\n",
    "\n",
    "Preserve data integrity: Your data migration solution should provide MD5 checksum on every file and assure all metadata and access controls migrate to the new environment.\n",
    "\n",
    "Avoid sunk costs: File data migrations are a lot of heaving lifting. Your data migration solution should include automatic parallelization at every level for elastic\n",
    "scaling and the ability to migrate petabytes of data seamlessly and reliably.\n",
    "\n",
    "Reduce downtime: It is recommended that your data migration solution runs multiple iterations for more efficient cutovers.\n",
    "\n",
    "Komprise and data migration\n",
    "Komprise Elastic Data Migration is included in the Komprise Intelligent Data Management platform or is available standalone. Designed for cloud migrations and NAS migrations, with Komprise Elastic Data Migration you can run, monitor, and manage hundreds of data migrations faster than ever at a fraction of the cost. Learn more about Komprise Smart Data Migrations.\n",
    "\n",
    "Data migration and the cloud\n",
    "As unstructured data continues to grow exponentially, organizations struggle to control costs for file data storage. Many are turning to the cloud to scale and manage spend. However, choosing the right files to move can be challenging as there can easily be billions of files. Many enterprises have over 1 PB of data, which represents roughly 3 billion files. This unstructured data is growing exponentially and resides in multi-vendor storage silos for access by various applications and departments.\n",
    "For these reasons, organizations often lack visibility into file data and are making decisions in the dark. To be agile and competitive, IT teams must evolve storage management to become a holistic data management strategy. The right approach to data migration and the cloud for file and object data is to use analytics in cloud data management:\n",
    "\n",
    "Understand your data patterns\n",
    "Plan using a cost model\n",
    "Use data to drive stakeholder buy-in\n",
    "Eliminate user disruption\n",
    "Create a systematic plan for ongoing data management\n",
    "Read the eBook: 5 Ways to Use Analytics for Cloud Data Migrations\n",
    "\n",
    "Top data migration challenges\n",
    "Businesses today are looking at modernizing storage and moving to a multi-cloud strategy. As they evolve to faster, flash-based Network Attached Storage (NAS) and the cloud, migrating data into these environments can be challenging. The goal is to migrate large production data sets quickly, without errors, and without disruption to user productivity.\n",
    "\n",
    "The top cloud data migration challenges are:\n",
    "\n",
    "How do you manage cloud data migrations without downtime?\n",
    "How can you automate cloud data migrations to eliminate manual effort?\n",
    "How can you ensure all the permissions, ACLs, metadata are copied correctly during a cloud data migration so you can access the data in the cloud as files?\n",
    "You can overcome these challenges with some planning and automation that preserves file-based access both from on-premises and the cloud.\n",
    "\n",
    "Data migration tools\n",
    "Free Tools: Require a lot of babysitting and are not reliable for migrating large volumes of data.\n",
    "Point Data Migration Solutions: Have complex legacy architectures and create sunk costs.\n",
    "Komprise Elastic Data Migration: Makes cloud data migrations simple, fast, reliable and eliminates sunk costs since you continue to use Komprise after the migration. Komprise is the only solution that gives you the option to cut 70%+ cloud storage costs by placing cold data in Object classes while maintaining file metadata so it can be promoted in the cloud as files when needed.\"\"\"\n",
    "\n",
    "# Get vector of test text\n",
    "test_vector = model.infer_vector(word_tokenize(doc))\n",
    "\n",
    "# Get all CV's that match\n",
    "best_matches = model.dv.similar_by_vector(test_vector)\n",
    "\n",
    "#See all matches\n",
    "print(best_matches)\n",
    "print(best_matches[0][0])\n",
    "# Print the text of the best match\n",
    "# To analyse if there match is decent or not.\n",
    "print(tagged_data[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fbc881801efdacea0e7b752b334635ef300e91ebf40f63c0f3e5b421e2de0c51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
